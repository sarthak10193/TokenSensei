{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e3b77",
   "metadata": {},
   "source": [
    "## GOAL\n",
    "<p>\n",
    "Build a system, which when given an english alphabet, can generate new baby from it and suggest\n",
    "</p>\n",
    "\n",
    "### How\n",
    "Build a character level language model using LSTM  that will learn to predict the next char given the\n",
    "characters it has seen before. This is a divergence from the\n",
    "bigram character model we saw before where we just model the counts in a frequentist approach. \n",
    "\n",
    "<span class=\"mark\">y = argmax(P(next char  | curr char ))</span>\n",
    "\n",
    "<p>\n",
    "In the LSTM world, because we are able to observe and train\n",
    "for 'long term dependecies'\n",
    "we'll model for\n",
    "<p>\n",
    "</br>\n",
    "\n",
    "<span class=\"mark\">(next char | prev time steps #of char)</span>\n",
    "\n",
    "<p>\n",
    "Description: generative char-level LSTM\n",
    "Type: LSTM\n",
    "Input: A list of baby names\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f3e1a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b48bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T07:53:29.447656Z",
     "start_time": "2023-02-02T07:53:20.254831Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 23:53:20.495267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "! rm -rf ./logs/\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed39162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T07:53:39.594929Z",
     "start_time": "2023-02-02T07:53:29.466042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f12afa53e081a8f0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f12afa53e081a8f0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a640b95",
   "metadata": {},
   "source": [
    "## Dataset Prep\n",
    "* The goal of this section is to prepare a `tf.BatchDataset` where the characters have been mapped to indexes using a vocab and tf.TextVectorizer()\n",
    "* A a proper formulation has been done on what the Input and the Targets will look like under each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc681f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T07:53:39.640283Z",
     "start_time": "2023-02-02T07:53:39.604605Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample names: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "total #of names: 32033\n",
      "sampled sequence: emma.olivia.ava.isab\n",
      "\n",
      "\n",
      "Vocab: ['d', 'o', 'i', 'g', 'e', 'r', 'w', 'v', 'y', 'b', 'k', 'l', 'u', 't', 'a', 'h', 'c', 'n', 'z', 'f', 's', 'm', '.', 'j', 'q', 'p', 'x']\n",
      "Vocab Size: 27\n"
     ]
    }
   ],
   "source": [
    "CURR_DIR = '' # os.path.dirname(__file__)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LEN = 12\n",
    "CHAR_EMBEDDING_SIZE = 256\n",
    "\n",
    "names_f_path = os.path.join(CURR_DIR, \"names.txt\")\n",
    "with open(names_f_path, 'r') as f:\n",
    "    names = f.readlines()\n",
    "    names_list = [x.strip() for x in names]\n",
    "\n",
    "ds = '.'.join(names_list)\n",
    "char_vocab = list(set(''.join(ds)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n",
    "print(f\"sample names: {names_list[:10]}\")\n",
    "print(f\"total #of names: {len(names_list)}\")\n",
    "print(f\"sampled sequence: {ds[:20]}\")\n",
    "print(f\"\\n\\nVocab: {char_vocab}\")\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf18b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T23:06:42.526697Z",
     "start_time": "2023-01-21T23:06:42.521340Z"
    }
   },
   "source": [
    "### Inputs and Targets for LSTM\n",
    "The table below shows that each sequence of length 10 fed in and the subsequent target sequence that the model will need at the end to predict the accuracy of prediction over next char.\n",
    "\n",
    "<table style=\"width:100%; text-align: center;\">\n",
    "    <tr>\n",
    "        <th style=\"background-color: #f2f2f2;\">Input Sequence</th>\n",
    "        <th style=\"background-color: #f2f2f2;\">Target Sequence</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 3px solid #ddd; padding: 8px;\">[s, a, r, t, h, a, k, ., j, i]</td>\n",
    "        <td style=\"border: 3px solid #ddd; padding: 8px;\">[a, r, t, h, a, k, ., j, i, n]</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 3px solid #ddd; padding: 8px;\">[r, t, h, a, k, ., j, i, n, a]</td>\n",
    "        <td style=\"border: 3px solid #ddd; padding: 8px;\">[t, h, a, k, ., j, i, n, a, l]</td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td style=\"border: 3px solid #ddd; padding: 8px;\">... continue</td>\n",
    "        <td style=\"border: 3px solid #ddd; padding: 8px;\"> ... continue</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* **Notice** below that there is an extra element at Position 0 encoded by the `StringLookup class` that is repsonsible for indexing the `[UNK]` token at 0th index of the char--> int lookup table\n",
    "* A a result the `EmbeddingLayer` will need to have `input_dim = vocab_size + 1`\n",
    "\n",
    "---\n",
    "\n",
    "<span class=\"mark\">Key Learning: Be always be rigourous with reading Tensorlfow documentation or using LLMs for QnA</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fab0ea6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T21:56:34.225894Z",
     "start_time": "2023-01-28T21:56:33.887244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e', 'm', 'm', 'a', '.', 'o', 'l', 'i', 'v', 'i', 'a', '.', 'a'], ['v', 'a', '.', 'i', 's', 'a', 'b', 'e', 'l', 'l', 'a', '.', 's'], ['o', 'p', 'h', 'i', 'a', '.', 'c', 'h', 'a', 'r', 'l', 'o', 't']]\n",
      "tf.Tensor([ 1 26 15 22  5 26 16], shape=(7,), dtype=int64)\n",
      "tf.Tensor([b's' b'a' b'r' b't' b'h' b'a' b'k'], shape=(7,), dtype=string)\n",
      "tf.Tensor([b'[UNK]' b'r' b'u' b'l' b'z' b'r' b'k'], shape=(7,), dtype=string)\n",
      "sequence_int_matrix shape: (17549, 13)\n",
      "Y shape: (12,)\n",
      "X shape: (12,)\n"
     ]
    }
   ],
   "source": [
    "ds_split = list(ds)\n",
    "ds_split_sequences = []\n",
    "for i in range(0, len(ds_split), SEQUENCE_LEN + 1):\n",
    "    ds_split_sequences.append(ds_split[i: i + SEQUENCE_LEN + 1])\n",
    "\n",
    "print(ds_split_sequences[:3])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The vocabulary for the layer must be either supplied on\n",
    "construction or learned via adapt()\n",
    "This layer translates a set of arbitrary strings into integer output\n",
    "via a table-based vocabulary lookup.\n",
    "This layer will perform no splitting or transformation of input strings.\n",
    "For a layer than can split and tokenize natural language,\n",
    "see the TextVectorization layer.\n",
    "\n",
    "\"\"\"\n",
    "char_encoder_layer = tf.keras.layers.StringLookup(vocabulary=char_vocab, )\n",
    "char_decoder_layer = tf.keras.layers.StringLookup(vocabulary=char_encoder_layer.get_vocabulary(), output_mode=\"int\", invert=True)\n",
    "test_encode = char_encoder_layer(list(\"sarthak\"))\n",
    "print(test_encode)\n",
    "print(char_decoder_layer(test_encode))\n",
    "\n",
    "c = tf.constant([ 0, 15, 23, 14,  6, 15, 16])\n",
    "print(char_decoder_layer(c))\n",
    "\n",
    "# return a tf.tensor of ints\n",
    "sequence_int_matrix = char_encoder_layer(np.array(ds_split_sequences[:-1]))\n",
    "print(f'sequence_int_matrix shape: {sequence_int_matrix.shape}')\n",
    "\n",
    "X_dataset = sequence_int_matrix[:, :-1]\n",
    "Y_dataset = sequence_int_matrix[: , 1:]\n",
    "X = tf.data.Dataset.from_tensor_slices(X_dataset)\n",
    "Y = tf.data.Dataset.from_tensor_slices(Y_dataset)\n",
    "print(f\"Y shape: {Y.element_spec.shape}\")\n",
    "print(f\"X shape: {X.element_spec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5984616a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T21:56:48.693845Z",
     "start_time": "2023-01-28T21:56:48.598130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchDatasetXY: <MapDataset element_spec=(TensorSpec(shape=(64, 12), dtype=tf.int64, name=None), TensorSpec(shape=(64, 12), dtype=tf.int64, name=None))>\n",
      "Input : tf.Tensor([[b'k' b'l' b'y' b'n' b'.' b'b' b'e' b'l' b'l' b'a' b'.' b'c']], shape=(1, 12), dtype=string)\n",
      "Target: tf.Tensor([[b'l' b'y' b'n' b'.' b'b' b'e' b'l' b'l' b'a' b'.' b'c' b'l']], shape=(1, 12), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "tf_batch_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    sequence_int_matrix).shuffle(27).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    \"\"\"\n",
    "    Creates Input and labels such that labels sequences are shifted by 1 position to represent next prediction\n",
    "    \"\"\"\n",
    "    input_text = sequence[:, :-1]\n",
    "    target_text = sequence[:, 1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "tf_batch_dataset = tf_batch_dataset.map(split_input_target) # will now contain 2 TensorSpecs \n",
    "\n",
    "print(f\"BatchDatasetXY: {tf_batch_dataset}\")\n",
    "for input_example, target_example in tf_batch_dataset.take(1):\n",
    "    print(\"Input :\", char_decoder_layer(input_example.numpy())[:1])\n",
    "    print(\"Target:\", char_decoder_layer(target_example.numpy())[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc657484",
   "metadata": {},
   "source": [
    "* you can notice that the name such as `sophia`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959e920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T20:32:53.769558Z",
     "start_time": "2023-01-21T20:32:53.766932Z"
    }
   },
   "source": [
    "## Building the Model [Level 1]\n",
    "Here we will use the `Sequential API` of tensoflow to train a model in the most simplistic and quick way. The `Sequential API` has it's limitation of us:\n",
    "* Not being able to control forward passes by overriding `call` \n",
    "* Not being able to write custom training step using `train_step()`\n",
    "* Not being able to write `custom loss functions and or metrics` that you would like to compute during the said training_step\n",
    "* Not being able to differentiate `training, masking = True vs False` when you want different behavior during train and test\n",
    " * Not being able to differentiate trainable vs non-trainable variables and selectively apply gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8d096",
   "metadata": {},
   "source": [
    "###  Character Embeddings\n",
    "While the mapping of characters to idx is great in the previous section, we'll need to:\n",
    "1. encode the indexes into some representation. We'll be going forwad with the **Character Embeddings** by using the `tf.keras.layers.Embedding` layer. **Note** this is not mandatory, we could have total used \n",
    "2. for which we'll be creating an embedding layer using `tf.keras.layers.Embedding` of `dim=256`\n",
    "3. The weights between the <u>input layer & embedding layer</u> will be learned through backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46561fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T20:35:21.508088Z",
     "start_time": "2023-01-21T20:35:21.503376Z"
    }
   },
   "source": [
    "###  Hidden LSTM Layer \n",
    "* number of `units` determines the number of **timesteps/recurrensces/iterations/ == sequence_len** over the sequence fed into the LSTM cell. \n",
    "* If we have set `return_state=True` and `return_sequences=True`, we'll have:\n",
    "    * access to additional tensors ie ...`return_sequences=True`gives the entire sequence of `length=num_units` and `return_state=True` returns the final state in addition to the sequences\n",
    "    * **Won't be able to use the Sequential API**, since it requires that all layers in a Sequential model should have a single output tensor.  For multi-output layers, use the functional API.\n",
    "    \n",
    "* The LSTM layer has weights in `multiples of 4` s.t the the shape of the weights matrix will be `(batch_size, sequence_len*4)`. This corresponds to the gates and cell states. \n",
    "    * **forget gate:** decide what to forget from the cell state\n",
    "    * **update gate:** what from the input to let through, to update the cell state\n",
    "    * **output gate:** what to probabilistically pass as the output\n",
    "    * **cell state:** running cell state. Cell state is responsible for tracking long-term depedencies and the gates control the information that enters or leaves it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "530d10c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T02:36:51.553795Z",
     "start_time": "2023-01-29T02:36:51.289318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CharLevel_GenerativeLangModel_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " CharacterEmbeddingLayer-256  (None, None, 256)        7168      \n",
      "  (Embedding)                                                    \n",
      "                                                                 \n",
      " LSTMLayer_WithSEQLEN-12 (LS  (None, None, 12)         12912     \n",
      " TM)                                                             \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, None, 28)          364       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,444\n",
      "Trainable params: 20,444\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RETURN_SEQUENCES = True\n",
    "\n",
    "model = tf.keras.Sequential(name=\"CharLevel_GenerativeLangModel_LSTM\")\n",
    " \n",
    "embedding_layer= tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=CHAR_EMBEDDING_SIZE, \n",
    "        name=f\"CharacterEmbeddingLayer-{CHAR_EMBEDDING_SIZE}\"\n",
    "    )\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "lstm_layer= tf.keras.layers.LSTM(\n",
    "        units=SEQUENCE_LEN,\n",
    "        name=f\"LSTMLayer_WithSEQLEN-{SEQUENCE_LEN}\", \n",
    "        return_sequences=RETURN_SEQUENCES\n",
    ")\n",
    "model.add(lstm_layer)\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(vocab_size+1)\n",
    "model.add(dense_layer)\n",
    "\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=loss_fn,\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True)]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce835049",
   "metadata": {},
   "source": [
    "### Gut check on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "671be793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T02:36:52.294941Z",
     "start_time": "2023-01-29T02:36:52.291063Z"
    }
   },
   "outputs": [],
   "source": [
    "if not RETURN_SEQUENCES:\n",
    "    print(f\"RETURN_SEQUENCES is set to: {RETURN_SEQUENCES} \")\n",
    "    for input_example, target_example in tf_batch_dataset.take(1):\n",
    "        # here the input and target examples are idxs which will be converted into embeddings by the \n",
    "        # EmbeddingLookup layer into a char--> vector represations of dim = 256\n",
    "        x = model(input_example)\n",
    "        print(x.shape)\n",
    "        print(tf.reduce_sum(x[:1, ])) # checking if the model is outputting probabilities\n",
    "        print(np.sum(x[:1, ].numpy()))\n",
    "        print(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003532d",
   "metadata": {},
   "source": [
    "<span class=\"mark\">Setting `RETURN_SEQUENCES=True` return the entire predicted sequence from the LSTM. <span class=\"girk\">This is desired since</span> you would like to know what the next char prediction was for each char in the sequence and not just the final character in the sequence</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bd294ff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T02:36:53.935786Z",
     "start_time": "2023-01-29T02:36:53.878491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETURN_SEQUENCES is set to: True \n",
      "output shape: (64, 12, 28), label shape: (64, 12)\n",
      "Loss on this example is: (64, 12)\n",
      "0.02270607277750969\n",
      "!!!! We are using Logits (Un-normalized probs) !!!! \n"
     ]
    }
   ],
   "source": [
    "if RETURN_SEQUENCES :\n",
    "    print(f\"RETURN_SEQUENCES is set to: {RETURN_SEQUENCES} \")\n",
    "    for input_example, target_example in tf_batch_dataset.take(1):\n",
    "        x = model(input_example)\n",
    "        print(f\"output shape: {x.shape}, label shape: {target_example.shape}\")\n",
    "        is_probs = tf.reduce_sum(x[:1,:1, :]).numpy()\n",
    "        test_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_example, logits=x)\n",
    "        print(f\"Loss on this example is: {test_loss.shape}\") \n",
    "        print(float(is_probs))\n",
    "        if is_probs == 1.0:\n",
    "            print(f\"the output is a probabilities array of len {vocab_size + 1}\")\n",
    "        else:\n",
    "            print(\"!!!! We are using Logits (Un-normalized probs) !!!! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d38e9",
   "metadata": {},
   "source": [
    "**Notice** that the LSTM-DENSE weights matrix is of shape `sequence_length, dense_units`. Here, each column of len `SEQUENCE_LEN=12` represents the final `carry state Ct` of the LSTM after the entire sequence of len `SEQUENCE_LEN` has been fed in. \n",
    "\n",
    "![ alt text for screen readers](LSTMCell.png \"Text to show on mouseover\") .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ca569753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-29T02:36:55.367041Z",
     "start_time": "2023-01-29T02:36:55.348138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding layer weight matrix: (28, 256)\n",
      "LSTM layer weight matrix: (256, 48)\n",
      "Dense layer weight matrix: (12, 28)\n"
     ]
    }
   ],
   "source": [
    "print(f\"embedding layer weight matrix: {embedding_layer.get_weights()[0].shape}\")\n",
    "print(f\"LSTM layer weight matrix: {lstm_layer.get_weights()[0].shape}\")\n",
    "print(f\"Dense layer weight matrix: {dense_layer.get_weights()[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95709131",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4485ff46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-30T04:26:37.511681Z",
     "start_time": "2023-01-30T04:23:38.513895Z"
    },
    "cell_style": "center"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 2.2132 - sparse_categorical_crossentropy: 2.2132\n",
      "Epoch 2/60\n",
      "274/274 [==============================] - 2s 8ms/step - loss: 2.2129 - sparse_categorical_crossentropy: 2.2129\n",
      "Epoch 3/60\n",
      "274/274 [==============================] - 2s 9ms/step - loss: 2.2126 - sparse_categorical_crossentropy: 2.2126\n",
      "Epoch 4/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2124 - sparse_categorical_crossentropy: 2.2124\n",
      "Epoch 5/60\n",
      "274/274 [==============================] - 3s 9ms/step - loss: 2.2121 - sparse_categorical_crossentropy: 2.2121\n",
      "Epoch 6/60\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 2.2120 - sparse_categorical_crossentropy: 2.2120\n",
      "Epoch 7/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2116 - sparse_categorical_crossentropy: 2.2116\n",
      "Epoch 8/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2115 - sparse_categorical_crossentropy: 2.2115\n",
      "Epoch 9/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2112 - sparse_categorical_crossentropy: 2.2112\n",
      "Epoch 10/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2110 - sparse_categorical_crossentropy: 2.2110\n",
      "Epoch 11/60\n",
      "274/274 [==============================] - 2s 9ms/step - loss: 2.2108 - sparse_categorical_crossentropy: 2.2108\n",
      "Epoch 12/60\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 2.2107 - sparse_categorical_crossentropy: 2.2107\n",
      "Epoch 13/60\n",
      "274/274 [==============================] - 2s 9ms/step - loss: 2.2104 - sparse_categorical_crossentropy: 2.2104\n",
      "Epoch 14/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2101 - sparse_categorical_crossentropy: 2.2101\n",
      "Epoch 15/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2099 - sparse_categorical_crossentropy: 2.2099\n",
      "Epoch 16/60\n",
      "274/274 [==============================] - 2s 8ms/step - loss: 2.2096 - sparse_categorical_crossentropy: 2.2096\n",
      "Epoch 17/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2096 - sparse_categorical_crossentropy: 2.2096\n",
      "Epoch 18/60\n",
      "274/274 [==============================] - 3s 9ms/step - loss: 2.2093 - sparse_categorical_crossentropy: 2.2093\n",
      "Epoch 19/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2091 - sparse_categorical_crossentropy: 2.2091\n",
      "Epoch 20/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2089 - sparse_categorical_crossentropy: 2.2089\n",
      "Epoch 21/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2088 - sparse_categorical_crossentropy: 2.2088\n",
      "Epoch 22/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2087 - sparse_categorical_crossentropy: 2.2087\n",
      "Epoch 23/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2085 - sparse_categorical_crossentropy: 2.2085\n",
      "Epoch 24/60\n",
      "274/274 [==============================] - 3s 9ms/step - loss: 2.2082 - sparse_categorical_crossentropy: 2.2082\n",
      "Epoch 25/60\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 2.2080 - sparse_categorical_crossentropy: 2.2080\n",
      "Epoch 26/60\n",
      "274/274 [==============================] - 2s 9ms/step - loss: 2.2079 - sparse_categorical_crossentropy: 2.2079\n",
      "Epoch 27/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2078 - sparse_categorical_crossentropy: 2.2078\n",
      "Epoch 28/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2076 - sparse_categorical_crossentropy: 2.2076\n",
      "Epoch 29/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2074 - sparse_categorical_crossentropy: 2.2074\n",
      "Epoch 30/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2073 - sparse_categorical_crossentropy: 2.2073\n",
      "Epoch 31/60\n",
      "274/274 [==============================] - 2s 8ms/step - loss: 2.2071 - sparse_categorical_crossentropy: 2.2071\n",
      "Epoch 32/60\n",
      "274/274 [==============================] - 3s 9ms/step - loss: 2.2070 - sparse_categorical_crossentropy: 2.2070\n",
      "Epoch 33/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2068 - sparse_categorical_crossentropy: 2.2068\n",
      "Epoch 34/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2066 - sparse_categorical_crossentropy: 2.2066\n",
      "Epoch 35/60\n",
      "274/274 [==============================] - 3s 9ms/step - loss: 2.2064 - sparse_categorical_crossentropy: 2.2064\n",
      "Epoch 36/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2063 - sparse_categorical_crossentropy: 2.2063\n",
      "Epoch 37/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2062 - sparse_categorical_crossentropy: 2.2062\n",
      "Epoch 38/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2060 - sparse_categorical_crossentropy: 2.2060\n",
      "Epoch 39/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2057 - sparse_categorical_crossentropy: 2.2057\n",
      "Epoch 40/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2055 - sparse_categorical_crossentropy: 2.2055\n",
      "Epoch 41/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2054 - sparse_categorical_crossentropy: 2.2054\n",
      "Epoch 42/60\n",
      "274/274 [==============================] - 4s 16ms/step - loss: 2.2053 - sparse_categorical_crossentropy: 2.2053\n",
      "Epoch 43/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2052 - sparse_categorical_crossentropy: 2.2052\n",
      "Epoch 44/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2049 - sparse_categorical_crossentropy: 2.2049\n",
      "Epoch 45/60\n",
      "274/274 [==============================] - 3s 10ms/step - loss: 2.2048 - sparse_categorical_crossentropy: 2.2048\n",
      "Epoch 46/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2048 - sparse_categorical_crossentropy: 2.2048\n",
      "Epoch 47/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2044 - sparse_categorical_crossentropy: 2.2044\n",
      "Epoch 48/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2043 - sparse_categorical_crossentropy: 2.2043\n",
      "Epoch 49/60\n",
      "274/274 [==============================] - 4s 15ms/step - loss: 2.2041 - sparse_categorical_crossentropy: 2.2041\n",
      "Epoch 50/60\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 2.2040 - sparse_categorical_crossentropy: 2.2040\n",
      "Epoch 51/60\n",
      "274/274 [==============================] - 3s 13ms/step - loss: 2.2039 - sparse_categorical_crossentropy: 2.2039\n",
      "Epoch 52/60\n",
      "274/274 [==============================] - 3s 12ms/step - loss: 2.2036 - sparse_categorical_crossentropy: 2.2036\n",
      "Epoch 53/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2035 - sparse_categorical_crossentropy: 2.2035\n",
      "Epoch 54/60\n",
      "274/274 [==============================] - 3s 9ms/step - loss: 2.2034 - sparse_categorical_crossentropy: 2.2034\n",
      "Epoch 55/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2032 - sparse_categorical_crossentropy: 2.2032\n",
      "Epoch 56/60\n",
      "274/274 [==============================] - 4s 14ms/step - loss: 2.2031 - sparse_categorical_crossentropy: 2.2031\n",
      "Epoch 57/60\n",
      "274/274 [==============================] - 4s 13ms/step - loss: 2.2028 - sparse_categorical_crossentropy: 2.2028\n",
      "Epoch 58/60\n",
      "274/274 [==============================] - 3s 13ms/step - loss: 2.2028 - sparse_categorical_crossentropy: 2.2028\n",
      "Epoch 59/60\n",
      "274/274 [==============================] - 3s 11ms/step - loss: 2.2027 - sparse_categorical_crossentropy: 2.2027\n",
      "Epoch 60/60\n",
      "274/274 [==============================] - 2s 7ms/step - loss: 2.2025 - sparse_categorical_crossentropy: 2.2025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa03ae7c850>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_batch_dataset,\n",
    "    batch_size=None,\n",
    "    epochs=60,\n",
    "    verbose='auto',\n",
    "    callbacks=[tensorboard_callback],\n",
    "    validation_split=0.0,\n",
    "    validation_data=None,\n",
    "    shuffle=True,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=None,\n",
    "    validation_steps=None,\n",
    "    validation_batch_size=None,\n",
    "    validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "    workers=2,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42c424",
   "metadata": {},
   "source": [
    "## Building the Model [Level 2]\n",
    "* Here we use the tensorflow `Functional API` to solve the same problem as above. Keras is based on the **core-principle of iterative disclosure and access to complexity without falling off the cliff.** \n",
    "* We'll use the following more complex features:\n",
    "    * extending the `tf.keras.Model` class to write our own forward pass by over-riding the `call()` method\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336868b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T22:38:31.942446Z",
     "start_time": "2023-01-28T22:38:31.910880Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "class CharLSTM(tf.keras.Model):\n",
    "    def train_step(self, input_data):\n",
    "        x,y = input_data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # this is equivalent to calling model(inputs, training=True)\n",
    "            # which intern uses the call() method. \n",
    "            # you could certainly not rely on defacto forward pass and chose to over-ride the call() method\n",
    "            y_pred = self(x, training=True) \n",
    "            loss = loss_fn(y, x)\n",
    "            print(loss)\n",
    "            \n",
    "model = CharLSTM()  \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=loss_fn,\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "\n",
    "tf.keras.utils.plot_model(model, \"charLSTM.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f833f8bf",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "2. Create a mulit-layer lstm rather than just 1 LSTM layer\n",
    "4. Think about sequence strucutre and if therea are other ways to formulate the problem\n",
    "5. what does masking do in `TextVectorizer` and `Embedding Layers`\n",
    "6. when to use `TextVectorizer` vs `StringLookup`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd5ef47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T22:50:17.201462Z",
     "start_time": "2023-01-21T22:50:17.197423Z"
    }
   },
   "source": [
    "## Key Learnings\n",
    "* the difference between `Dataset.from_tensor_slices()` and `Dataset.from_tensors()`. The `from_tensor_slices` method will slice the input data along the first dimesion of the input and will remove the mention of the first dimesion\n",
    "* `TextVectorization Layer` is strict with the datatype you can call adapt on. works with tf.data.Dataset or np.array\n",
    "* Keras is built on the **core principle of iterative exposure of compexity**. Therefore are many ways of training the model: \n",
    "    * Using the `Sequential API` and directly calling `model.compile()` and` model.fit()` . Here `model.fit()` is responsible for running the forward pass\n",
    "    * Sub-classing `tf.keras.Model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "217d2f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T22:52:17.979132Z",
     "start_time": "2023-01-21T22:52:17.969035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1200, 10), 48)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[10]*10]*1200)\n",
    "a.shape, 1200%64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f03f3f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T22:53:27.812576Z",
     "start_time": "2023-01-21T22:53:27.806422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(64, 10), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_batched_a = tf.data.Dataset.from_tensor_slices(a).batch(64, drop_remainder=True)\n",
    "tf_batched_a \n",
    "# notice how the mention of 1200 has been removed and the remaining \n",
    "# rows that won't fit the batch size will get dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e951b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "234.339px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
